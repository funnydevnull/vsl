To run/compile test classes:

SerializingTest:
================

From vsl/src dir:

javac vsl/test/SerializingTest.java

To run it:

java vsl.test.SerializingTest put mykey myval
java vsl.test.SerializingTest put mykey2 myval2

then 

java vsl.test.SerializingTest read 

If we use MultiValueMap from apache need to include classpath:

javac -cp .:../lib/commons-collections-3.2.1.jar  vsl/test/SerializingTest.java 



TestCore1
=========

Basic test of core functionality.

To run try (last two args optional):

java vsl.test.TestCore1 store <filename> [data_string] [numChunks]

Example: 

java vsl.test.TestCore1 store vslDB howdy 12

will generate an Entry with one Version and numChunks data chunks (there's a
default value) and store them in a vslMMBackend which stores itself to
filename.  NOTE: will OVERWRITE file at <filename>!!

To read back in and print out the file:

java vsl.test.TestCore1 read <filename> 



ByteHashSpeedTest
=================

Some test code to determine how to quickly rechunk a file.  

There are three basic commands: <testByte>, <chunkSame>, <chunkDiff>

testByte
--------

java vsl.test.ByteHashSpeedTest testByte <str1> <str2> ...

Tests our doubly-linked list of bytes (ByteDLL) implementation that allows us
to quickly match a byte sequence to a list of stored byte arrays.  Note that
performance is actually mostly due to a prestruct we have in this class that
vastly speeds up our search.

Try for instance:

java vsl.test.ByteHashSpeedTest testByte heya heyb heee hapo yaho

Note: for consistent results the strings should all be the SAME LENGTH (this is
not a bug as this is the relevant use case when considering file token
identifiers -- beginTokens). 

Also for efficiency reasons we do very litle error checking but it is assumed
that both the input strings and the search strings are greater than 3 in
length.

chunkSame
---------

Another tester tries chunking a file then rechunking it using various
techniques (hashing chunk identifiers, etc...).  This test has a lot of old
code in it that we probably don't need anymore.  The final rechunking is done
with our efficient struct.

Call it as:

java vsl.test.ByteHashSpeedTest rechunkSame <filename> <chunkSize> <beginTokenSize>

Note: should have beginTokenSize < chunkSize

For instance:

java vsl.test.ByteHashSpeedTest rechunkSame test_files/large.pdf 10000 100

chunkDiff
---------

Chunk two different versions of the file and compare the chunking.  We first
chunk <file1> and use it to generate a table of tokens and associated chunks.
We then check <file2> and see how many chunks from our chunking of <file1> we can find.  Note we do not actually chunk <file2>.  
This code can be used to test speed/efficiency of different chunk lengths,
etc...

Call it using:

java vsl.test.ByteHashSpeedTest rechunkDiff <orignal file> <new file version> <chunkSize> <beginTokenSize>

Note: should have beginTokenSize < chunkSize.  These refer of coure to the original chunking.

For instance:

java vsl.test.ByteHashSpeedTest rechunkSame test_files/version1.pdf test_files/version2.pdf 10000 100


FileChunkingTest
================

This test file chunking now USING vsl core.

To run cd to vsl/src and execute as below.

Chunk a file (in this case vsl_deisng_notes-1.tex) and store in the vsl DB-file mydb:

java vsl.test.FileChunkingTest create mydb  ../test/input/version/vsl_design_notes-1.tex

NOTE: chunking params currently hard-coded (i.e. size of chunk and begin
token).  Should make them args.

Since VSL doesn't yet support reading out entries we can't directly read or
reconstruct a file but instead we can access it using the printMap method in
the vslMMBackend but this requires using the TestCore1 class.

java vsl.test.TestCore1 read mydb [numBytes]

By default this method only shows the first 100 bytes of each data entry but by
providing an arg this can be increased.


